{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking org.deeplearning4j:deeplearning4j-core:0.6.0 for download\n",
      "Preparing to fetch from:\n",
      "-> file:/tmp/toree_add_deps7974131878688600591/\n",
      "-> https://repo1.maven.org/maven2\n",
      "-> New file at /tmp/toree_add_deps7974131878688600591/https/repo1.maven.org/maven2/org/deeplearning4j/deeplearning4j-core/0.6.0/deeplearning4j-core-0.6.0.jar\n",
      "Marking com.databricks:spark-csv_2.10:1.4.0 for download\n",
      "Preparing to fetch from:\n",
      "-> file:/tmp/toree_add_deps7974131878688600591/\n",
      "-> https://repo1.maven.org/maven2\n",
      "-> New file at /tmp/toree_add_deps7974131878688600591/https/repo1.maven.org/maven2/com/databricks/spark-csv_2.10/1.4.0/spark-csv_2.10-1.4.0.jar\n",
      "-> New file at /tmp/toree_add_deps7974131878688600591/https/repo1.maven.org/maven2/com/univocity/univocity-parsers/1.5.1/univocity-parsers-1.5.1.jar\n",
      "-> New file at /tmp/toree_add_deps7974131878688600591/https/repo1.maven.org/maven2/org/apache/commons/commons-csv/1.1/commons-csv-1.1.jar\n"
     ]
    }
   ],
   "source": [
    "%AddDeps org.deeplearning4j deeplearning4j-core 0.6.0\n",
    "%AddDeps com.databricks spark-csv_2.10 1.4.0 --transitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "/*import org.apache.spark.mllib.tree.GradientBoostedTrees\n",
    "import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n",
    "import org.apache.spark.mllib.tree.model.GradientBoostedTreesModel\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "// Load and parse the data file.\n",
    "val data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_libsvm_data.txt\")\n",
    "// Split the data into training and test sets (30% held out for testing)\n",
    "val splits = data.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, testData) = (splits(0), splits(1))\n",
    "\n",
    "// Train a GradientBoostedTrees model.\n",
    "// The defaultParams for Regression use SquaredError by default.\n",
    "val boostingStrategy = BoostingStrategy.defaultParams(\"Regression\")\n",
    "boostingStrategy.numIterations = 3 // Note: Use more iterations in practice.\n",
    "boostingStrategy.treeStrategy.maxDepth = 5\n",
    "// Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "boostingStrategy.treeStrategy.categoricalFeaturesInfo = Map[Int, Int]()\n",
    "\n",
    "val model = GradientBoostedTrees.train(trainingData, boostingStrategy)\n",
    "\n",
    "// Evaluate model on test instances and compute test error\n",
    "val labelsAndPredictions = testData.map { point =>\n",
    "  val prediction = model.predict(point.features)\n",
    "  (point.label, prediction)\n",
    "}\n",
    "val testMSE = labelsAndPredictions.map{ case(v, p) => math.pow((v - p), 2)}.mean()\n",
    "println(\"Test Mean Squared Error = \" + testMSE)\n",
    "println(\"Learned regression GBT model:\\n\" + model.toDebugString)\n",
    "\n",
    "// Save and load model\n",
    "model.save(sc, \"target/tmp/myGradientBoostingRegressionModel\")\n",
    "val sameModel = GradientBoostedTreesModel.load(sc,\n",
    "  \"target/tmp/myGradientBoostingRegressionModel\")*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "val sqlContext = SQLContext.getOrCreate(sc)\n",
    "import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"./dat/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "| id|cat1|cat2|\n",
      "+---+----+----+\n",
      "|  1|   A|   B|\n",
      "|  2|   A|   B|\n",
      "|  5|   A|   B|\n",
      "| 10|   B|   B|\n",
      "| 11|   A|   B|\n",
      "| 13|   A|   B|\n",
      "| 14|   A|   A|\n",
      "| 20|   A|   B|\n",
      "| 23|   A|   B|\n",
      "| 24|   A|   B|\n",
      "| 25|   A|   B|\n",
      "| 33|   A|   B|\n",
      "| 34|   B|   A|\n",
      "| 41|   B|   A|\n",
      "| 47|   A|   A|\n",
      "| 48|   A|   A|\n",
      "| 49|   A|   B|\n",
      "| 51|   A|   A|\n",
      "| 52|   A|   A|\n",
      "| 55|   A|   A|\n",
      "+---+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\", \"cat1\", \"cat2\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val (catCols, contCols) = df.columns.partition(_.contains(\"cat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(cat1, cat2, cat3, cat4, cat5, cat6, cat7, cat8, cat9, cat10, cat11, cat12, cat13, cat14, cat15, cat16, cat17, cat18, cat19, cat20, cat21, cat22, cat23, cat24, cat25, cat26, cat27, cat28, cat29, cat30, cat31, cat32, cat33, cat34, cat35, cat36, cat37, cat38, cat39, cat40, cat41, cat42, cat43, cat44, cat45, cat46, cat47, cat48, cat49, cat50, cat51, cat52, cat53, cat54, cat55, cat56, cat57, cat58, cat59, cat60, cat61, cat62, cat63, cat64, cat65, cat66, cat67, cat68, cat69, cat70, cat71, cat72, cat73, cat74, cat75, cat76, cat77, cat78, cat79, cat80, cat81, cat82, cat83, cat84, cat85, cat86, cat87, cat88, cat89, cat90, cat91, cat92, cat93, cat94, cat95, cat96, cat97, cat98, cat99, cat100, cat101, cat102, cat103, cat104, cat105, cat106, cat107, cat108, cat109, cat1..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(catCols.length)\n",
    "catCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(id, cont1, cont2, cont3, cont4, cont5, cont6, cont7, cont8, cont9, cont10, cont11, cont12, cont13, cont14, loss)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(contCols.length)\n",
    "contCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
